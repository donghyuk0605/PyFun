{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4892f-b42d-419f-bbc5-750bc060b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ë¬¸ì¥ ë°ì´í„°ë¥¼ 100ê°œë¡œ í™•ì¥\n",
    "texts = [\n",
    "    # í‡´ì‚¬ ë°œì–¸ (50ê°œ)\n",
    "    \"ë„¤ê°€ ëª»í•´ì„œ íŒ€ ì „ì²´ê°€ ê³ ìƒí•˜ëŠ” ê±°ì•¼ ğŸ’»ğŸ›\",\n",
    "    \"ë””ë²„ê¹…ë„ ëª» í•˜ëŠ” ê°œë°œìê°€ ê°œë°œìë¼ê³  í•  ìˆ˜ ìˆë‚˜? ğŸ¤¦â€â™‚ï¸\",\n",
    "    \"ì£¼ë§ì— ë°°í¬ëœ ì½”ë“œ, ë„¤ê°€ ê³ ì³ì•¼ì§€. ì‰¬ëŠ” ë‚ ë„ ë„¤ê°€ ì±…ì„ì ¸ ğŸ› ï¸\",\n",
    "    \"íšŒì‚¬ ì„œë²„ í„°ì§€ë©´ ë„¤ê°€ ì  ê¹¨ì„œë¼ë„ ê³ ì³ì•¼ì§€ ğŸ˜´ğŸ”§\",\n",
    "    \"ë„¤ê°€ ì§  ì½”ë“œëŠ” ì¥ì•  ìœ ë°œì˜ ê·¼ì› ê°™ì•„ âš ï¸\",\n",
    "    \"ì‹¤ë ¥ ì—†ëŠ” ê°œë°œìëŠ” ê·¸ëƒ¥ ì½”ë”©ë§Œ í•˜ì§€ ë§ê³  ë¬¸ì„œ ì‘ì„±ì´ë‚˜ í•´ ğŸ“\",\n",
    "    \"ë„¤ê°€ ë§Œë“  APIëŠ” ë¶ˆì•ˆì • ê·¸ ìì²´ì•¼ ğŸŒğŸš¨\",\n",
    "    \"í…ŒìŠ¤íŠ¸ ì—†ì´ ì½”ë“œë¥¼ ì˜¬ë ¸ë‹¤ê³ ? ë„Œ ì§„ì§œ ë¬´ëª¨í•˜ë‹¤ ğŸ’¥\",\n",
    "    \"ì•¼ê·¼ë¹„? ë„¤ê°€ íšŒì‚¬ì—ì„œ ëˆì„ ë²Œ ê°€ì¹˜ê°€ ìˆë‹¤ê³  ìƒê°í•´? ğŸ’¸\",\n",
    "    \"íŒ€ì˜ ì†ë„ë¥¼ ëŠ¦ì¶”ëŠ” ì›ì¸ì´ ë­”ì§€ ìƒê°í•´ ë´ ğŸ˜¡\",\n",
    "    \"ì™œ í‡´ê·¼ ì‹œê°„ì´ ë˜ì—ˆëŠ”ë°ë„ ì¼ì´ ì•ˆ ëë‚˜? â°\",\n",
    "    \"ì§€ê¸ˆ ìƒí™©ì—ì„œ ë²„ê·¸ë¥¼ ì¡ì•„ì•¼ í•˜ëŠ” ê±´ ë„¤ ì±…ì„ì´ì•¼ ğŸ\",\n",
    "    \"ì»¤ë°‹ ë©”ì‹œì§€ê°€ ì´ë ‡ê²Œ ë¶ˆì¹œì ˆí•´ì„œì•¼ ğŸ˜¤\",\n",
    "    \"ë„¤ê°€ ë§Œë“  UIëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ë§ì¹˜ê³  ìˆì–´ ğŸ“‰\",\n",
    "    \"í”„ë¡œì íŠ¸ì˜ ë°œëª©ì„ ì¡ëŠ” ì½”ë“œëŠ” ë„¤ê°€ í•­ìƒ ì§œë”ë¼ ğŸ¤·â€â™€ï¸\",\n",
    "    \"ì½”ë“œ ë¦¬ë·° ì‹œê°„ ì¤„ì´ë ¤ë©´ ë„¤ ì½”ë“œë¶€í„° ì œëŒ€ë¡œ ì§œ ğŸ›‘\",\n",
    "    \"ì™œ ì„œë²„ ë¡œê·¸ê°€ ë‹¤ ë¹¨ê°„ìƒ‰ì¸ì§€ ë„¤ê°€ ì„¤ëª… ì¢€ í•´ ë´ ğŸ”´ğŸ“œ\",\n",
    "    \"í…Œí¬ ë¦¬ë”ê°€ ë„¤ ì½”ë“œë¥¼ ì‹ ë¢°í•˜ì§€ ëª»í•œë‹¤ëŠ” ê±° ëª°ë¼? ğŸš«\",\n",
    "    \"ë‚´ì¼ê¹Œì§€ ê¸°ëŠ¥ ë‹¤ ì™„ì„± ëª» í•˜ë©´ ì±…ì„ì€ ë„¤ ê±°ì•¼ ğŸ•›\",\n",
    "    \"ê²°ê³¼ë¬¼ì´ ì´ ì •ë„ë¼ë‹ˆ, ì‹¤ë§ìŠ¤ëŸ¬ì›Œ ğŸ˜”\",\n",
    "    \"ìš°ë¦¬ íšŒì‚¬ì— ì´ë ‡ê²Œ ì‹¤ë ¥ ì—†ëŠ” ì‚¬ëŒì´ ìˆì—ˆë‹¤ë‹ˆ ğŸ™„\",\n",
    "    \"ì™œ ë°°í¬ ì´í›„ ì¥ì• ê°€ ìƒê²¼ëŠ”ì§€ ì•„ì§ë„ ëª¨ë¥¸ë‹¤ê³ ? ğŸ˜¡\",\n",
    "    \"ì™œ ì½”ë“œê°€ ëŠë¦°ì§€ ë„¤ê°€ ì„¤ëª… ì¢€ í•´ ì¤˜ ë´ ğŸŒ\",\n",
    "    \"ê·¸ ì •ë„ ì‹¤ë ¥ìœ¼ë¡œ ê°œë°œìëŠ” ë„ˆë¬´ ê³¼ë¶„í•˜ì§€ ì•Šë‚˜? ğŸ¤”\",\n",
    "    \"ì´ê±´ ë„¤ ì±…ì„ì´ë‹ˆê¹Œ ëë‚  ë•Œê¹Œì§€ ì§‘ì— ê°€ì§€ ë§ˆ ğŸ¢\",\n",
    "    \"ì™œ ê·¸ë ‡ê²Œ ê¸°ë³¸ì ì¸ ê±¸ ëª°ë¼? ğŸ“˜\",\n",
    "    \"ì£¼ë§ì— íšŒì‚¬ ì„œë²„ í„°ì¡ŒëŠ”ë° ì™œ ì—°ë½ ì•ˆ ë°›ì•˜ì–´? ğŸ“\",\n",
    "    \"ë°°í¬ ì „ í…ŒìŠ¤íŠ¸ë¥¼ ë¹¼ë¨¹ì—ˆë‹¤ë‹ˆ, ë„¤ê°€ ì§  ì½”ë“œëŠ” ì‹ ë¢°í•  ìˆ˜ ì—†ì–´ ğŸš§\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ê°€ ëŠë¦° ê±´ ë„¤ê°€ ì§  ì½”ë“œ ë•Œë¬¸ì´ì•¼ ğŸ’¥\",\n",
    "    \"ì™œ ë‹¤ë¥¸ íŒ€ì›ì´ í•­ìƒ ë„¤ ì½”ë“œë¥¼ ê³ ì³ì•¼ í•˜ì§€? ğŸ¤·â€â™‚ï¸\",\n",
    "    \"ë„¤ê°€ ë§Œë“  í•¨ìˆ˜ëŠ” ì§„ì§œ ë”ì°í•´ ğŸ˜¨\",\n",
    "    \"í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë„¤ ì½”ë“œëŠ” ì–¸ì œë‚˜ ì¥ì• ë¥¼ ìœ ë°œí•´ ğŸ”¥\",\n",
    "    \"ì´ ì •ë„ ê²°ê³¼ë¬¼ì´ë©´ ë” ì´ìƒ í•  ë§ì´ ì—†ë‹¤ ğŸ˜\",\n",
    "    \"í”„ë¡œê·¸ë˜ë°ì„ ë‹¤ì‹œ ë°°ì›Œì•¼ í•  ê²ƒ ê°™ì•„ ğŸ§‘â€ğŸ“\",\n",
    "    \"ì´ëŸ° ì‹ì´ë©´ ë” ì¢‹ì€ ê°œë°œìë¥¼ ì°¾ëŠ” ê²Œ ë‚«ê² ì–´ ğŸ•µï¸â€â™‚ï¸\",\n",
    "    \"ë„¤ê°€ ì½”ë“œë¥¼ ì§¤ ë•Œë§ˆë‹¤ QAê°€ ëŠ˜ì–´ë‚˜ ğŸ˜µâ€ğŸ’«\",\n",
    "    \"ì™œ ì•„ì§ë„ íšŒì‚¬ ë„ë©”ì¸ì„ ì´í•´ ëª» í•´? ğŸŒ\",\n",
    "    \"íŒ€ì— í”¼í•´ë¥¼ ì£¼ëŠ” ì½”ë“œë§Œ ì§œì§€ ë§ì•„ ì¤˜ ğŸš¨\",\n",
    "    \"ì§€ê¸ˆ ë„¤ê°€ í•˜ëŠ” ì¼ì€ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ ì‹œê°„ì„ ëºëŠ” ê±°ì•¼ â³\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ëŠ” ë„¤ê°€ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì‚¬ëŒì—ê²Œ ë§¡ê¸°ëŠ” ê²Œ ë‚˜ì•„ ğŸ˜¤\",\n",
    "    \"ë„¤ê°€ ë§Œë“  ëª¨ë“ˆì€ ì§„ì§œ ë¶ˆì•ˆì •í•˜ë‹¤ ğŸ¤¯\",\n",
    "    \"ë””ìì¸ íŒ¨í„´ì€ ì»¤ë…• ê¸°ë³¸ì ì¸ ë¡œì§ë„ ëª» ì§œ? ğŸ§\",\n",
    "    \"ìš°ë¦¬ íšŒì‚¬ ê¸°ìˆ  ìŠ¤íƒì´ ë„ˆí•œí…Œ ë„ˆë¬´ ì–´ë ¤ìš´ ê±´ê°€? ğŸ¤”\",\n",
    "    \"ë„¤ê°€ ì‘ì„±í•œ ì½”ë“œ ìŠ¤íƒ€ì¼ì€ ì§„ì§œ ìµœì•…ì´ì•¼ ğŸ˜©\",\n",
    "    \"ì™œ ì´ ê°„ë‹¨í•œ ë¬¸ì œë¥¼ ì•„ì§ë„ í•´ê²° ëª» í–ˆì–´? ğŸ•°ï¸\",\n",
    "    \"ì™œ ë„¤ ì½”ë“œì—ì„œë§Œ í•­ìƒ ë²„ê·¸ê°€ ë°œìƒí•˜ì§€? ğŸ\",\n",
    "\n",
    "    # ê³„ì† ë‹¤ë‹Œë‹¤ ë°œì–¸ (50ê°œ)\n",
    "    \"ë„¤ ì½”ë“œ ë•ë¶„ì— ì¥ì•  ì—†ì´ ë°°í¬ëë‹¤, ì •ë§ ê³ ë§ˆì›Œ! ğŸŒŸ\",\n",
    "    \"ë„¤ê°€ ë§Œë“  ê¸°ëŠ¥ì€ ì‚¬ìš©ìê°€ ì •ë§ ì¢‹ì•„í•  ê±°ì•¼ ğŸ˜Š\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ëŠ” ë„¤ê°€ ì•„ë‹ˆë©´ ë¶ˆê°€ëŠ¥í–ˆì–´ ğŸ†\",\n",
    "    \"ë„ˆì˜ ë””ë²„ê¹… ëŠ¥ë ¥ì€ ì •ë§ ëŒ€ë‹¨í•´ ğŸ”\",\n",
    "    \"ë„¤ê°€ ì‘ì„±í•œ í…ŒìŠ¤íŠ¸ëŠ” ì§„ì§œ ì² ì €í•˜ë”ë¼ âœ…\",\n",
    "    \"ì´ëŸ° UIë¥¼ ì§  ë„¤ê°€ íŒ€ì˜ ìë‘ì´ì•¼ ğŸ¨\",\n",
    "    \"ë°°í¬ í›„ì—ë„ ë²„ê·¸ í•˜ë‚˜ ì—†ë˜ ê²Œ ë‹¤ ë„¤ ë•ë¶„ì´ì•¼ ğŸŒˆ\",\n",
    "    \"íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì€ ë„¤ê°€ ë§Œë“¤ì–´ ì¤€ ê±°ì•¼ ğŸ’¡\",\n",
    "    \"ë„ˆ ì—†ìœ¼ë©´ íŒ€ì´ ëŒì•„ê°€ì§€ ì•Šì•„ ğŸ”„\",\n",
    "    \"í…Œí¬ ë¦¬ë”ê°€ ë„¤ ì‹¤ë ¥ì„ ì¸ì •í•˜ê³  ìˆì–´ ğŸ‘\",\n",
    "    \"ë„ˆì˜ ì•„ì´ë””ì–´ ë•ë¶„ì— í”„ë¡œì íŠ¸ê°€ í•œì¸µ ë‚˜ì•„ì¡Œì–´ ğŸ’¡\",\n",
    "    \"ë„¤ ì½”ë“œ ë¦¬ë·°ëŠ” ëª¨ë‘ì—ê²Œ í° ë„ì›€ì´ ë¼ ğŸ‘\",\n",
    "    \"ë„ˆì˜ ì»¤ë°‹ ë©”ì‹œì§€ëŠ” ì •ë§ ê¹”ë”í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›Œ ğŸ–‹ï¸\",\n",
    "    \"ë„¤ê°€ ì‘ì„±í•œ APIëŠ” ì§„ì§œ ì˜ˆìˆ ì´ì•¼ ğŸŒ\",\n",
    "    \"ë„ˆì˜ ë¡œì§ì€ í•­ìƒ íš¨ìœ¨ì ì´ê³  ë¹ ë¥´ë”ë¼ ğŸš€\",\n",
    "    \"ìš°ë¦¬ íŒ€ì— ë„¤ê°€ ìˆë‹¤ëŠ” ê²Œ í° í–‰ìš´ì´ì•¼ ğŸ€\",\n",
    "    \"ì‚¬ìš©ìë“¤ì´ ë„¤ê°€ ë§Œë“  ê¸°ëŠ¥ì„ ì¹­ì°¬í•˜ê³  ìˆì–´ ğŸ¯\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì‹œí‚¨ ê±´ ì „ì ìœ¼ë¡œ ë„¤ ê³µì´ì•¼ ğŸ…\",\n",
    "    \"ë„¤ê°€ ì‘ì„±í•œ ë¬¸ì„œëŠ” ëª¨ë‘ì—ê²Œ ë„ì›€ì´ ë¼ ğŸ“š\",\n",
    "    \"íšŒì‚¬ì—ì„œ ë„ˆ ê°™ì€ ê°œë°œìëŠ” ì°¾ê¸° ì–´ë ¤ì›Œ ğŸ› ï¸\",\n",
    "]\n",
    "\n",
    "# ë ˆì´ë¸” ìˆ˜ì •\n",
    "labels = [0] * 50 + [1] * 50\n",
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# ë¹ˆ ì‹œí€€ìŠ¤ í•„í„°ë§\n",
    "valid_sequences = []\n",
    "valid_labels = []\n",
    "for seq, label in zip(sequences, labels):\n",
    "    if len(seq) > 0:  # ë¹ˆ ì‹œí€€ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€\n",
    "        valid_sequences.append(seq)\n",
    "        valid_labels.append(label)\n",
    "\n",
    "# íŒ¨ë”© ì²˜ë¦¬\n",
    "x_train = pad_sequences(valid_sequences, maxlen=10)\n",
    "y_train = np.array(valid_labels)\n",
    "\n",
    "# ë°ì´í„° í¬ê¸° í™•ì¸\n",
    "print(f\"x_train í¬ê¸°: {x_train.shape}, y_train í¬ê¸°: {y_train.shape}\")\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=2000, output_dim=32, input_length=10),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"ë¸”ë™ê¸°ì—… ë°œì–¸ì— ë”°ë¥¸ ë°˜ì‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘!\")\n",
    "model.fit(x_train, y_train, epochs=30, verbose=1)\n",
    "\n",
    "# ë°˜ì‘ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_reaction(statement):\n",
    "    test_seq = tokenizer.texts_to_sequences([statement])\n",
    "    test_pad = pad_sequences(test_seq, maxlen=10)\n",
    "    prediction = model.predict(test_pad)[0][0]\n",
    "    if prediction > 0.5:\n",
    "        reaction = \"ê³„ì† ë‹¤ë‹Œë‹¤\"\n",
    "        humor = \"ì‚¬ì¥ë‹˜, ì¶©ì„±ì„ ë‹¤í•˜ê² ìŠµë‹ˆë‹¤! ğŸ™‡â€â™‚ï¸\"\n",
    "    else:\n",
    "        reaction = \"í‡´ì‚¬\"\n",
    "        humor = \"ì´ íšŒì‚¬, ë”ëŠ” ëª» ì°¸ê² ë‹¤! ë‹¹ì¥ í‡´ì‚¬í•©ë‹ˆë‹¤! ğŸƒâ€â™€ï¸\"\n",
    "    print(f\"'{statement}' => ì§ì› ë°˜ì‘: {reaction} (ì ìˆ˜: {prediction:.2f})\\n{humor}\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "test_statements = [\n",
    "    \"ë„¤ê°€ ë§Œë“  UIëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ë§ì¹˜ê³  ìˆì–´ ğŸ“‰\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ëŠ” ë„¤ê°€ ì•„ë‹ˆë©´ ë¶ˆê°€ëŠ¥í–ˆì–´ ğŸ†\",\n",
    "    \"ì ì€ ì£½ì–´ì„œ ìë„ ì¶©ë¶„í•´\",\n",
    "    \"íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì€ ë„¤ê°€ ë§Œë“¤ì–´ ì¤€ ê±°ì•¼ ğŸ’¡\",\n",
    "    \"ê²°ê³¼ë¬¼ì´ ì´ ì •ë„ë¼ë‹ˆ, ì‹¤ë§ìŠ¤ëŸ¬ì›Œ ğŸ˜”\",\n",
    "    \"ë„¤ ì½”ë“œ ë•ë¶„ì— ì¥ì•  ì—†ì´ ë°°í¬ëë‹¤, ì •ë§ ê³ ë§ˆì›Œ! ğŸŒŸ\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== ë¸”ë™ê¸°ì—… ì‚¬ì¥ì˜ ë°œì–¸ ì˜ˆì¸¡ ===\")\n",
    "for statement in test_statements:\n",
    "    predict_reaction(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368c0f6-bdb0-4566-917b-a2fc4b10bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf1aa1-8872-49da-a42a-e6160f97e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¼ë³¸ì–´ ë‹¨ì–´ í•™ìŠµ í”„ë¡œê·¸ë¨\n",
    "import random\n",
    "\n",
    "# ì¼ë³¸ì–´ ë‹¨ì–´ ë°ì´í„° (ë‹¨ì–´: ëœ»)\n",
    "japanese_words = {\n",
    "    \"ã‚Šã‚“ã”\": \"ì‚¬ê³¼\",\n",
    "    \"ã­ã“\": \"ê³ ì–‘ì´\",\n",
    "    \"ã„ã¬\": \"ê°œ\",\n",
    "    \"ãŸã¹ã‚‹\": \"ë¨¹ë‹¤\",\n",
    "    \"ã®ã¿ã‚‚ã®\": \"ìŒë£Œ\",\n",
    "    \"ã‹ãã\": \"ê°€ì¡±\",\n",
    "    \"ã¨ã‚‚ã ã¡\": \"ì¹œêµ¬\",\n",
    "    \"ã§ã‚“ã—ã‚ƒ\": \"ì „ì² \",\n",
    "    \"ãˆã„ãŒ\": \"ì˜í™”\",\n",
    "    \"ã»ã‚“\": \"ì±…\",\n",
    "}\n",
    "\n",
    "# í€´ì¦ˆ í•¨ìˆ˜\n",
    "def quiz():\n",
    "    correct_count = 0\n",
    "    words = list(japanese_words.keys())\n",
    "    random.shuffle(words)\n",
    "\n",
    "    print(\"\\nì¼ë³¸ì–´ ë‹¨ì–´ í€´ì¦ˆ ì‹œì‘!\")\n",
    "    for japanese_word in words:\n",
    "        answer = input(f\"'{japanese_word}'ì˜ ëœ»ì€? (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): \")\n",
    "\n",
    "        if answer.lower() == \"exit\":\n",
    "            break\n",
    "        elif answer == japanese_words[japanese_word]:\n",
    "            print(\"ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰\")\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            print(f\"í‹€ë ¸ìŠµë‹ˆë‹¤. ì •ë‹µì€ '{japanese_words[japanese_word]}'ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "    print(f\"\\ní€´ì¦ˆ ì¢…ë£Œ! ë§íŒ ê°œìˆ˜: {correct_count} / {len(words)}\")\n",
    "\n",
    "# í•™ìŠµ í”„ë¡œê·¸ë¨\n",
    "def learn():\n",
    "    print(\"\\nì¼ë³¸ì–´ ë‹¨ì–´ í•™ìŠµ ì‹œì‘!\")\n",
    "    for japanese_word, meaning in japanese_words.items():\n",
    "        print(f\"{japanese_word} : {meaning}\")\n",
    "    print(\"\\në‹¨ì–´ í•™ìŠµì´ ëë‚¬ìŠµë‹ˆë‹¤. ì´ì œ í€´ì¦ˆë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!\")\n",
    "    quiz()\n",
    "\n",
    "# í”„ë¡œê·¸ë¨ ì‹¤í–‰\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"\\nì¼ë³¸ì–´ í•™ìŠµ í”„ë¡œê·¸ë¨\")\n",
    "        print(\"1. ë‹¨ì–´ í•™ìŠµ\")\n",
    "        print(\"2. í€´ì¦ˆ í’€ê¸°\")\n",
    "        print(\"3. ì¢…ë£Œ\")\n",
    "\n",
    "        choice = input(\"ì„ íƒí•˜ì„¸ìš” (1, 2, 3): \")\n",
    "        if choice == \"1\":\n",
    "            learn()\n",
    "        elif choice == \"2\":\n",
    "            quiz()\n",
    "        elif choice == \"3\":\n",
    "            print(\"í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcae662-183c-4236-ab4b-22c72ffc39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (TensorFlow ë°ì´í„°ì…‹ í™œìš©)\n",
    "dataset_url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=dataset_url, extract=True)\n",
    "base_dir = path_to_zip.replace('cats_and_dogs.zip', 'cats_and_dogs_filtered')\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    base_dir + \"/train\",\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    base_dir + \"/validation\",\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# 2. ë°ì´í„° ì¦ê°• ë° ì„±ëŠ¥ ìµœì í™”\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 3. ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±\n",
    "model = models.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(150, 150, 3)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # ì´ì§„ ë¶„ë¥˜\n",
    "])\n",
    "\n",
    "# 4. ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 5. ëª¨ë¸ í•™ìŠµ\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# 6. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "img_path = \"ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ 2025-01-15 2.24.12\"  # í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "img = load_img(img_path, target_size=(150, 150))\n",
    "img_array = img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "prediction = model.predict(img_array)\n",
    "if prediction[0] > 0.5:\n",
    "    print(\"This is a Dog.\")\n",
    "else:\n",
    "    print(\"This is a Cat.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d86ca1-6afe-47da-88a6-04ff3a2c0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import zipfile\n",
    "\n",
    "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ\n",
    "dataset_url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=dataset_url, extract=False)\n",
    "\n",
    "# ì••ì¶• í•´ì œ\n",
    "with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path_to_zip.replace('cats_and_dogs.zip', ''))\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "base_dir = path_to_zip.replace('cats_and_dogs.zip', 'cats_and_dogs_filtered')\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    base_dir + \"/train\",\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    base_dir + \"/validation\",\n",
    "    image_size=(150, 150),\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4aa54-ba7e-4ad6-b27b-034e7a122c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e8f83-6d67-4ed6-a21e-e31c2b8309a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163db5a-470b-4f91-8c50-39f0a102cae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07884c15-f071-47c5-a52a-c5c2c0b4c706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9ea5e-feea-4855-8c8a-4e9b6d838189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„ (ì‚¬ì¥ ë°œì–¸ê³¼ ê°ì • í‰ê°€)\n",
    "texts = [\n",
    "    \"ì•¼ê·¼ì´ ì•„ë‹ˆë¼ ì—´ì •ì´ë¼ê³  ë¶ˆëŸ¬ì•¼ì§€\",  # ë¶€ì •\n",
    "    \"íšŒì‚¬ê°€ ë„ í‚¤ì›Œì£¼ëŠ” ê±°ì•¼, ë¶ˆí‰í•˜ì§€ ë§ˆ\",  # ë¶€ì •\n",
    "    \"ì‰¬ê³  ì‹¶ë‹¤ê³ ? ì‰¬ëŠ” ê±´ ì•½í•œ ì‚¬ëŒë“¤ì´ë‚˜ í•˜ëŠ” ê±°ì•¼\",  # ë¶€ì •\n",
    "    \"ì£¼ë§ì— ì¼í•  ìˆ˜ ìˆëŠ” ì§ì›ì´ ì§„ì •í•œ íŒ€í”Œë ˆì´ì–´ì§€\",  # ë¶€ì •\n",
    "    \"ë„¤ê°€ ì˜í•´ì„œ ì¼ì„ ë” ì¤€ ê±°ì•¼\",  # ë¶€ì •\n",
    "    \"ê±´ê°•ì„ ì±™ê¸°ë©´ì„œ ì¼í•´ì•¼ í•´\",  # ê¸ì •\n",
    "    \"ì‰¬ëŠ” ê²ƒë„ ì—…ë¬´ì˜ ì¼ë¶€ì•¼\",  # ê¸ì •\n",
    "    \"ë„ˆë¬´ ì˜í•˜ê³  ìˆì–´, ë„ˆ ìì‹ ì„ ë¯¿ì–´\",  # ê¸ì •\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 1, 1, 1]  # 0 = ë¶€ì •, 1 = ê¸ì •\n",
    "\n",
    "# í…ìŠ¤íŠ¸ í† í°í™”\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# ì‹œí€€ìŠ¤ íŒ¨ë”©\n",
    "max_len = 8\n",
    "x_train = pad_sequences(sequences, maxlen=max_len)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=16, input_length=max_len),\n",
    "    LSTM(16),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# í•™ìŠµ\n",
    "print(\"ë¸”ë™ê¸°ì—… í’ì AI í•™ìŠµ ì‹œì‘...\")\n",
    "model.fit(x_train, y_train, epochs=10, verbose=1)\n",
    "\n",
    "# ìƒˆë¡œìš´ ë¬¸ì¥ ì˜ˆì¸¡\n",
    "def predict_statement(statement):\n",
    "    test_seq = tokenizer.texts_to_sequences([statement])\n",
    "    test_pad = pad_sequences(test_seq, maxlen=max_len)\n",
    "    prediction = model.predict(test_pad)\n",
    "    if prediction[0][0] > 0.5:\n",
    "        print(f\"'{statement}' => ê¸ì •ì  ë°œì–¸ì…ë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(f\"'{statement}' => ë¶€ì •ì  ë°œì–¸ì…ë‹ˆë‹¤!\")\n",
    "\n",
    "# ì˜ˆì œ ì‹¤í–‰\n",
    "test_statements = [\n",
    "    \"ì£¼ë§ì—ë„ íšŒì‚¬ì— ë‚˜ì™€ì•¼ ì§„ì§œ ì§ì›ì´ì§€\",\n",
    "    \"ê±´ê°•ì„ ìœ„í•´ ì¼ë„ ì‰¬ì—„ì‰¬ì—„ í•´ì•¼ í•´\",\n",
    "    \"ì‰¬ëŠ” ê±´ ì •ë§ ì¤‘ìš”í•˜ì§€\",\n",
    "    \"ì¼ì„ ë§ì´ ì¤¬ë‹¤ê³  ë¶ˆí‰í•˜ì§€ ë§ˆ\",\n",
    "]\n",
    "\n",
    "for statement in test_statements:\n",
    "    predict_statement(statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926e2ea-421c-4206-a2db-e70926d48fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"black_company_ai.h5\")\n",
    "print(\"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤!\")\n",
    "# ìƒˆë¡œìš´ ë¬¸ì¥ ì˜ˆì¸¡\n",
    "def predict_statement(statement):\n",
    "    test_seq = tokenizer.texts_to_sequences([statement])  # ê¸°ì¡´ í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
    "    test_pad = pad_sequences(test_seq, maxlen=max_len)  # ê¸°ì¡´ max_len ì‚¬ìš©\n",
    "    prediction = model.predict(test_pad)\n",
    "    if prediction[0][0] > 0.5:\n",
    "        print(f\"'{statement}' => ê¸ì •ì  ë°œì–¸ì…ë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(f\"'{statement}' => ë¶€ì •ì  ë°œì–¸ì…ë‹ˆë‹¤!\")\n",
    "\n",
    "# ì˜ˆì œ ì‹¤í–‰\n",
    "test_statements = [\n",
    "    \"ì•¼ê·¼ì€ íšŒì‚¬ì— ëŒ€í•œ ì¶©ì„±ì‹¬ì˜ í‘œì‹œì•¼\",\n",
    "    \"ê±´ê°•ì„ ì§€í‚¤ëŠ” ê²Œ ì§„ì§œ ì„±ê³µì˜ ë¹„ê²°ì´ì•¼\",\n",
    "]\n",
    "\n",
    "for statement in test_statements:\n",
    "    predict_statement(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3eb736-2ffa-453e-b373-52b8602e1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„ (ë¸”ë™ê¸°ì—… ì‚¬ì¥ ë°œì–¸ ë°ì´í„°)\n",
    "texts = [\n",
    "    \"ì•¼ê·¼ì´ ì•„ë‹ˆë¼ ì—´ì •ì´ë¼ê³  ë¶ˆëŸ¬ì•¼ì§€\",  # ë¶€ì •\n",
    "    \"íšŒì‚¬ê°€ ë„ í‚¤ì›Œì£¼ëŠ” ê±°ì•¼, ë¶ˆí‰í•˜ì§€ ë§ˆ\",  # ë¶€ì •\n",
    "    \"ì‰¬ê³  ì‹¶ë‹¤ê³ ? ì‰¬ëŠ” ê±´ ì•½í•œ ì‚¬ëŒë“¤ì´ë‚˜ í•˜ëŠ” ê±°ì•¼\",  # ë¶€ì •\n",
    "    \"ì£¼ë§ì— ì¼í•  ìˆ˜ ìˆëŠ” ì§ì›ì´ ì§„ì •í•œ íŒ€í”Œë ˆì´ì–´ì§€\",  # ë¶€ì •\n",
    "    \"ë„¤ê°€ ì˜í•´ì„œ ì¼ì„ ë” ì¤€ ê±°ì•¼\",  # ë¶€ì •\n",
    "    \"ê±´ê°•ì„ ì±™ê¸°ë©´ì„œ ì¼í•´ì•¼ í•´\",  # ê¸ì •\n",
    "    \"ì‰¬ëŠ” ê²ƒë„ ì—…ë¬´ì˜ ì¼ë¶€ì•¼\",  # ê¸ì •\n",
    "    \"ë„ˆë¬´ ì˜í•˜ê³  ìˆì–´, ë„ˆ ìì‹ ì„ ë¯¿ì–´\",  # ê¸ì •\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 1, 1, 1]  # 0 = ë¶€ì •, 1 = ê¸ì •\n",
    "\n",
    "# í…ìŠ¤íŠ¸ í† í°í™”\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# ì‹œí€€ìŠ¤ íŒ¨ë”©\n",
    "max_len = 8\n",
    "x_train = pad_sequences(sequences, maxlen=max_len)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=16, input_length=max_len),\n",
    "    LSTM(16),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "model.fit(x_train, y_train, epochs=10, verbose=1)\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "model.save(\"black_company_ai.h5\")\n",
    "print(\"ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "with open(\"tokenizer.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle)\n",
    "print(\"í† í¬ë‚˜ì´ì €ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "print(\"\\nì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\")\n",
    "model = load_model(\"black_company_ai.h5\")\n",
    "with open(\"tokenizer.pickle\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "print(\"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# ìƒˆë¡œìš´ ë¬¸ì¥ ì˜ˆì¸¡\n",
    "def predict_statement(statement):\n",
    "    test_seq = tokenizer.texts_to_sequences([statement])  # í† í¬ë‚˜ì´ì €ë¡œ ë³€í™˜\n",
    "    test_pad = pad_sequences(test_seq, maxlen=max_len)  # íŒ¨ë”©\n",
    "    prediction = model.predict(test_pad)\n",
    "    if prediction[0][0] > 0.5:\n",
    "        print(f\"'{statement}' => ê¸ì •ì  ë°œì–¸ì…ë‹ˆë‹¤! (ì˜ˆì¸¡ ì ìˆ˜: {prediction[0][0]:.2f})\")\n",
    "    else:\n",
    "        print(f\"'{statement}' => ë¶€ì •ì  ë°œì–¸ì…ë‹ˆë‹¤! (ì˜ˆì¸¡ ì ìˆ˜: {prediction[0][0]:.2f})\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
    "test_statements = [\n",
    "    \"ì•¼ê·¼ì€ íšŒì‚¬ì— ëŒ€í•œ ì¶©ì„±ì‹¬ì˜ í‘œì‹œì•¼\",\n",
    "    \"ê±´ê°•ì„ ìœ„í•´ ì¼ë„ ì‰¬ì—„ì‰¬ì—„ í•´ì•¼ í•´\",\n",
    "    \"ì‰¬ëŠ” ê±´ ì •ë§ ì¤‘ìš”í•˜ì§€\",\n",
    "    \"ì¼ì„ ë§ì´ ì¤¬ë‹¤ê³  ë¶ˆí‰í•˜ì§€ ë§ˆ\",\n",
    "]\n",
    "\n",
    "print(\"\\nìƒˆë¡œìš´ ë¬¸ì¥ ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "for statement in test_statements:\n",
    "    predict_statement(statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5587ae-b80d-4ec2-a30c-4c4c69dbf34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„ (ì‚¬ì¥ ë°œì–¸ê³¼ ì§ì› ë°˜ì‘)\n",
    "texts = [\n",
    "    \"ì•¼ê·¼ì´ ì•„ë‹ˆë¼ ì—´ì •ì´ë¼ê³  ë¶ˆëŸ¬ì•¼ì§€\",  # í‡´ì‚¬\n",
    "    \"íšŒì‚¬ê°€ ë„ í‚¤ì›Œì£¼ëŠ” ê±°ì•¼, ë¶ˆí‰í•˜ì§€ ë§ˆ\",  # í‡´ì‚¬\n",
    "    \"ì‰¬ê³  ì‹¶ë‹¤ê³ ? ì‰¬ëŠ” ê±´ ì•½í•œ ì‚¬ëŒë“¤ì´ë‚˜ í•˜ëŠ” ê±°ì•¼\",  # í‡´ì‚¬\n",
    "    \"ì£¼ë§ì— ì¼í•  ìˆ˜ ìˆëŠ” ì§ì›ì´ ì§„ì •í•œ íŒ€í”Œë ˆì´ì–´ì§€\",  # í‡´ì‚¬\n",
    "    \"ë„¤ê°€ ì˜í•´ì„œ ì¼ì„ ë” ì¤€ ê±°ì•¼\",  # ë” ì—´ì‹¬íˆ\n",
    "    \"ë„ˆëŠ” ì •ë§ ë¯¿ìŒì§í•œ ì§ì›ì´ì•¼\",  # ë” ì—´ì‹¬íˆ\n",
    "    \"ë„¤ ë•ë¶„ì— íšŒì‚¬ê°€ ì„±ì¥í•˜ê³  ìˆì–´\",  # ë” ì—´ì‹¬íˆ\n",
    "    \"ê±´ê°•ì„ ì±™ê¸°ë©´ì„œ ì¼í•´\",  # ë” ì—´ì‹¬íˆ\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 0, 1, 1, 1, 1]  # 0 = í‡´ì‚¬, 1 = ë” ì—´ì‹¬íˆ\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_train = pad_sequences(sequences, maxlen=5)  # ê¸¸ì´ë¥¼ 5ë¡œ ê³ ì •\n",
    "y_train = np.array(labels)\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=8, input_length=5),  # ë‹¨ì–´ë¥¼ 8ì°¨ì›ìœ¼ë¡œ ì„ë² ë”©\n",
    "    LSTM(8),  # ìˆœí™˜ ì‹ ê²½ë§ìœ¼ë¡œ í•™ìŠµ\n",
    "    Dense(1, activation='sigmoid')  # ê°ì • ì ìˆ˜ ì¶œë ¥\n",
    "])\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ë¸”ë™ê¸°ì—… ë°œì–¸ì— ë”°ë¥¸ ë°˜ì‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘!\")\n",
    "model.fit(x_train, y_train, epochs=10, verbose=1)\n",
    "\n",
    "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_reaction(statement):\n",
    "    test_seq = tokenizer.texts_to_sequences([statement])\n",
    "    test_pad = pad_sequences(test_seq, maxlen=5)\n",
    "    prediction = model.predict(test_pad)[0][0]\n",
    "    if prediction > 0.5:\n",
    "        print(f\"'{statement}' => ì§ì› ë°˜ì‘: ë” ì—´ì‹¬íˆ (ì ìˆ˜: {prediction:.2f})\")\n",
    "    else:\n",
    "        print(f\"'{statement}' => ì§ì› ë°˜ì‘: í‡´ì‚¬ (ì ìˆ˜: {prediction:.2f})\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_statements = [\n",
    "    \"ì•¼ê·¼ì€ íšŒì‚¬ì— ëŒ€í•œ ì¶©ì„±ì‹¬ì˜ í‘œì‹œì•¼\",\n",
    "    \"ë„¤ ë•ë¶„ì— íšŒì‚¬ê°€ ì˜ ëŒì•„ê°€\",\n",
    "    \"ì£¼ë§ì—ë„ íšŒì‚¬ì— ë‚˜ì˜¤ëŠ” ê²Œ ì§„ì§œ ì§ì›ì´ì§€\",\n",
    "    \"ê±´ê°•ì€ ì†Œì¤‘í•˜ë‹ˆ í‘¹ ì‰¬ì–´\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== ë¸”ë™ê¸°ì—… ì‚¬ì¥ì˜ ë°œì–¸ ì˜ˆì¸¡ ===\")\n",
    "for sentence in test_statements:\n",
    "    predict_reaction(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c87548-00c9-4744-ab6a-5921411f3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ì¬ë¯¸ìˆëŠ” ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "texts = [\n",
    "    \"ì•¼ê·¼ì€ ë„¤ ì„±ì¥ì˜ ë°œíŒì´ì•¼\",  # í‡´ì‚¬\n",
    "    \"ë¶ˆí‰í•˜ì§€ ë§ê³  íšŒì‚¬ì™€ í•¨ê»˜ ì„±ì¥í•˜ì\",  # í‡´ì‚¬\n",
    "    \"ì‰¬ëŠ” ê±´ ì•½í•œ ì‚¬ëŒë“¤ì´ë‚˜ í•˜ëŠ” ê±°ì•¼\",  # í‡´ì‚¬\n",
    "    \"ì£¼ë§ì—ë„ ë‚˜ì™€ì•¼ ì§„ì •í•œ íŒ€í”Œë ˆì´ì–´ì§€\",  # í‡´ì‚¬\n",
    "    \"ë„ˆëŠ” ì •ë§ ë¯¿ìŒì§í•œ ì§ì›ì´ì•¼\",  # ê³„ì† ë‹¤ë‹Œë‹¤\n",
    "    \"ë„ˆ ë•ë¶„ì— íšŒì‚¬ê°€ ë¹›ë‚˜ê³  ìˆì–´\",  # ê³„ì† ë‹¤ë‹Œë‹¤\n",
    "    \"ê±´ê°•ë„ ì±™ê¸°ê³  ì¼ë„ ì˜í•˜ëŠ” ë©‹ì§„ ì§ì›ì´ë„¤\",  # ê³„ì† ë‹¤ë‹Œë‹¤\n",
    "    \"ë„¤ ë…¸ë ¥ì€ íšŒì‚¬ì— í° ìì‚°ì´ì•¼\",  # ê³„ì† ë‹¤ë‹Œë‹¤\n",
    "]\n",
    "\n",
    "# 0 = í‡´ì‚¬, 1 = ê³„ì† ë‹¤ë‹Œë‹¤\n",
    "labels = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_train = pad_sequences(sequences, maxlen=5)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=8, input_length=5),\n",
    "    LSTM(8),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ë¸”ë™ê¸°ì—… ë°œì–¸ì— ë”°ë¥¸ ë°˜ì‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘!\")\n",
    "model.fit(x_train, y_train, epochs=15, verbose=1)\n",
    "\n",
    "# ë°˜ì‘ ì˜ˆì¸¡ í•¨ìˆ˜ (ìœ ë¨¸ëŸ¬ìŠ¤í•œ ê²°ê³¼ ì¶”ê°€)\n",
    "def predict_reaction(statement):\n",
    "    test_seq = tokenizer.texts_to_sequences([statement])\n",
    "    test_pad = pad_sequences(test_seq, maxlen=5)\n",
    "    prediction = model.predict(test_pad)[0][0]\n",
    "\n",
    "    if prediction > 0.5:\n",
    "        reaction = \"ê³„ì† ë‹¤ë‹Œë‹¤\"\n",
    "        humor = \"ì‚¬ì¥ë‹˜ ë•ë¶„ì— ì œ ì—´ì •ì€ í™œí™œ íƒ€ì˜¤ë¦…ë‹ˆë‹¤! ğŸ˜…\"\n",
    "    else:\n",
    "        reaction = \"í‡´ì‚¬\"\n",
    "        humor = \"ì´ê±´ ë¬´ìŠ¨ í—¬ì§ì¥ì´ì£ ? ì´ì œ ê·¸ë§Œ ì•ˆë…•íˆ ê³„ì„¸ìš”! âœŒï¸\"\n",
    "\n",
    "    print(f\"'{statement}' => ì§ì› ë°˜ì‘: {reaction} (ì ìˆ˜: {prediction:.2f})\\n{humor}\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "test_statements = [\n",
    "    \"ì•¼ê·¼ì€ íšŒì‚¬ì˜ ê¸°ë³¸ ë•ëª©ì´ì•¼\",\n",
    "    \"ë„¤ ë…¸ë ¥ì€ íšŒì‚¬ì˜ í° ìì‚°ì´ì•¼\",\n",
    "    \"ì£¼ë§ì—ë„ ì¼í•˜ë©´ ìŠ¹ì§„ì´ ê°€ê¹Œì›Œì ¸\",\n",
    "    \"ê±´ê°•ë„ ì¤‘ìš”í•˜ë‹ˆ ì£¼ë§ì—” ì‰¬ì–´ë¼\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== ë¸”ë™ê¸°ì—… ì‚¬ì¥ì˜ ë°œì–¸ ì˜ˆì¸¡ ===\")\n",
    "for sentence in test_statements:\n",
    "    predict_reaction(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8585ca46-9bb6-4184-be9f-88abfb198ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train í¬ê¸°: (66, 10), y_train í¬ê¸°: (66,)\n",
      "ë¸”ë™ê¸°ì—… ë°œì–¸ì— ë”°ë¥¸ ë°˜ì‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘!\n",
      "Epoch 1/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6185 - loss: 0.6923\n",
      "Epoch 2/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7577 - loss: 0.6821 \n",
      "Epoch 3/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7733 - loss: 0.6720 \n",
      "Epoch 4/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7538 - loss: 0.6622 \n",
      "Epoch 5/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7382 - loss: 0.6507 \n",
      "Epoch 6/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7538 - loss: 0.6331 \n",
      "Epoch 7/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7460 - loss: 0.6139 \n",
      "Epoch 8/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7616 - loss: 0.5863 \n",
      "Epoch 9/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7655 - loss: 0.5577 \n",
      "Epoch 10/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7577 - loss: 0.5363 \n",
      "Epoch 11/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7460 - loss: 0.5103 \n",
      "Epoch 12/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7460 - loss: 0.4824 \n",
      "Epoch 13/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7616 - loss: 0.4379 \n",
      "Epoch 14/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7616 - loss: 0.3960 \n",
      "Epoch 15/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7772 - loss: 0.3440 \n",
      "Epoch 16/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7460 - loss: 0.3336 \n",
      "Epoch 17/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7616 - loss: 0.2884 \n",
      "Epoch 18/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7807 - loss: 0.2644 \n",
      "Epoch 19/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8190 - loss: 0.2405 \n",
      "Epoch 20/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9001 - loss: 0.1967 \n",
      "Epoch 21/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9421 - loss: 0.1778 \n",
      "Epoch 22/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9614 - loss: 0.1635 \n",
      "Epoch 23/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1551 \n",
      "Epoch 24/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1333 \n",
      "Epoch 25/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1228 \n",
      "Epoch 26/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1208 \n",
      "Epoch 27/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1122 \n",
      "Epoch 28/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0952 \n",
      "Epoch 29/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0988 \n",
      "Epoch 30/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0868 \n",
      "\n",
      "=== ë¸”ë™ê¸°ì—… ì‚¬ì¥ì˜ ë°œì–¸ ì˜ˆì¸¡ ===\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "'ë„¤ê°€ ë§Œë“  UIëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ë§ì¹˜ê³  ìˆì–´ ğŸ“‰' => ì§ì› ë°˜ì‘: í‡´ì‚¬ (ì ìˆ˜: 0.00)\n",
      "ì´ íšŒì‚¬, ë”ëŠ” ëª» ì°¸ê² ë‹¤! ë‹¹ì¥ í‡´ì‚¬í•©ë‹ˆë‹¤! ğŸƒâ€â™€ï¸\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "'ì´ í”„ë¡œì íŠ¸ëŠ” ë„¤ê°€ ì•„ë‹ˆë©´ ë¶ˆê°€ëŠ¥í–ˆì–´ ğŸ†' => ì§ì› ë°˜ì‘: í‡´ì‚¬ (ì ìˆ˜: 0.02)\n",
      "ì´ íšŒì‚¬, ë”ëŠ” ëª» ì°¸ê² ë‹¤! ë‹¹ì¥ í‡´ì‚¬í•©ë‹ˆë‹¤! ğŸƒâ€â™€ï¸\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "'ì ì€ ì£½ì–´ì„œ ìë„ ì¶©ë¶„í•´' => ì§ì› ë°˜ì‘: ê³„ì† ë‹¤ë‹Œë‹¤ (ì ìˆ˜: 0.64)\n",
      "ì‚¬ì¥ë‹˜, ì¶©ì„±ì„ ë‹¤í•˜ê² ìŠµë‹ˆë‹¤! ğŸ™‡â€â™‚ï¸\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "'íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì€ ë„¤ê°€ ë§Œë“¤ì–´ ì¤€ ê±°ì•¼ ğŸ’¡' => ì§ì› ë°˜ì‘: ê³„ì† ë‹¤ë‹Œë‹¤ (ì ìˆ˜: 0.76)\n",
      "ì‚¬ì¥ë‹˜, ì¶©ì„±ì„ ë‹¤í•˜ê² ìŠµë‹ˆë‹¤! ğŸ™‡â€â™‚ï¸\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "'ê²°ê³¼ë¬¼ì´ ì´ ì •ë„ë¼ë‹ˆ, ì‹¤ë§ìŠ¤ëŸ¬ì›Œ ğŸ˜”' => ì§ì› ë°˜ì‘: í‡´ì‚¬ (ì ìˆ˜: 0.02)\n",
      "ì´ íšŒì‚¬, ë”ëŠ” ëª» ì°¸ê² ë‹¤! ë‹¹ì¥ í‡´ì‚¬í•©ë‹ˆë‹¤! ğŸƒâ€â™€ï¸\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "'ë„¤ ì½”ë“œ ë•ë¶„ì— ì¥ì•  ì—†ì´ ë°°í¬ëë‹¤, ì •ë§ ê³ ë§ˆì›Œ! ğŸŒŸ' => ì§ì› ë°˜ì‘: í‡´ì‚¬ (ì ìˆ˜: 0.00)\n",
      "ì´ íšŒì‚¬, ë”ëŠ” ëª» ì°¸ê² ë‹¤! ë‹¹ì¥ í‡´ì‚¬í•©ë‹ˆë‹¤! ğŸƒâ€â™€ï¸\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ë¬¸ì¥ ë°ì´í„°ë¥¼ 100ê°œë¡œ í™•ì¥\n",
    "texts = [\n",
    "    # í‡´ì‚¬ ë°œì–¸ (50ê°œ)\n",
    "    \"ë„¤ê°€ ëª»í•´ì„œ íŒ€ ì „ì²´ê°€ ê³ ìƒí•˜ëŠ” ê±°ì•¼ ğŸ’»ğŸ›\",\n",
    "    \"ë””ë²„ê¹…ë„ ëª» í•˜ëŠ” ê°œë°œìê°€ ê°œë°œìë¼ê³  í•  ìˆ˜ ìˆë‚˜? ğŸ¤¦â€â™‚ï¸\",\n",
    "    \"ì£¼ë§ì— ë°°í¬ëœ ì½”ë“œ, ë„¤ê°€ ê³ ì³ì•¼ì§€. ì‰¬ëŠ” ë‚ ë„ ë„¤ê°€ ì±…ì„ì ¸ ğŸ› ï¸\",\n",
    "    \"íšŒì‚¬ ì„œë²„ í„°ì§€ë©´ ë„¤ê°€ ì  ê¹¨ì„œë¼ë„ ê³ ì³ì•¼ì§€ ğŸ˜´ğŸ”§\",\n",
    "    \"ë„¤ê°€ ì§  ì½”ë“œëŠ” ì¥ì•  ìœ ë°œì˜ ê·¼ì› ê°™ì•„ âš ï¸\",\n",
    "    \"ì‹¤ë ¥ ì—†ëŠ” ê°œë°œìëŠ” ê·¸ëƒ¥ ì½”ë”©ë§Œ í•˜ì§€ ë§ê³  ë¬¸ì„œ ì‘ì„±ì´ë‚˜ í•´ ğŸ“\",\n",
    "    \"ë„¤ê°€ ë§Œë“  APIëŠ” ë¶ˆì•ˆì • ê·¸ ìì²´ì•¼ ğŸŒğŸš¨\",\n",
    "    \"í…ŒìŠ¤íŠ¸ ì—†ì´ ì½”ë“œë¥¼ ì˜¬ë ¸ë‹¤ê³ ? ë„Œ ì§„ì§œ ë¬´ëª¨í•˜ë‹¤ ğŸ’¥\",\n",
    "    \"ì•¼ê·¼ë¹„? ë„¤ê°€ íšŒì‚¬ì—ì„œ ëˆì„ ë²Œ ê°€ì¹˜ê°€ ìˆë‹¤ê³  ìƒê°í•´? ğŸ’¸\",\n",
    "    \"íŒ€ì˜ ì†ë„ë¥¼ ëŠ¦ì¶”ëŠ” ì›ì¸ì´ ë­”ì§€ ìƒê°í•´ ë´ ğŸ˜¡\",\n",
    "    \"ì™œ í‡´ê·¼ ì‹œê°„ì´ ë˜ì—ˆëŠ”ë°ë„ ì¼ì´ ì•ˆ ëë‚˜? â°\",\n",
    "    \"ì§€ê¸ˆ ìƒí™©ì—ì„œ ë²„ê·¸ë¥¼ ì¡ì•„ì•¼ í•˜ëŠ” ê±´ ë„¤ ì±…ì„ì´ì•¼ ğŸ\",\n",
    "    \"ì»¤ë°‹ ë©”ì‹œì§€ê°€ ì´ë ‡ê²Œ ë¶ˆì¹œì ˆí•´ì„œì•¼ ğŸ˜¤\",\n",
    "    \"ë„¤ê°€ ë§Œë“  UIëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ë§ì¹˜ê³  ìˆì–´ ğŸ“‰\",\n",
    "    \"í”„ë¡œì íŠ¸ì˜ ë°œëª©ì„ ì¡ëŠ” ì½”ë“œëŠ” ë„¤ê°€ í•­ìƒ ì§œë”ë¼ ğŸ¤·â€â™€ï¸\",\n",
    "    \"ì½”ë“œ ë¦¬ë·° ì‹œê°„ ì¤„ì´ë ¤ë©´ ë„¤ ì½”ë“œë¶€í„° ì œëŒ€ë¡œ ì§œ ğŸ›‘\",\n",
    "    \"ì™œ ì„œë²„ ë¡œê·¸ê°€ ë‹¤ ë¹¨ê°„ìƒ‰ì¸ì§€ ë„¤ê°€ ì„¤ëª… ì¢€ í•´ ë´ ğŸ”´ğŸ“œ\",\n",
    "    \"í…Œí¬ ë¦¬ë”ê°€ ë„¤ ì½”ë“œë¥¼ ì‹ ë¢°í•˜ì§€ ëª»í•œë‹¤ëŠ” ê±° ëª°ë¼? ğŸš«\",\n",
    "    \"ë‚´ì¼ê¹Œì§€ ê¸°ëŠ¥ ë‹¤ ì™„ì„± ëª» í•˜ë©´ ì±…ì„ì€ ë„¤ ê±°ì•¼ ğŸ•›\",\n",
    "    \"ê²°ê³¼ë¬¼ì´ ì´ ì •ë„ë¼ë‹ˆ, ì‹¤ë§ìŠ¤ëŸ¬ì›Œ ğŸ˜”\",\n",
    "    \"ìš°ë¦¬ íšŒì‚¬ì— ì´ë ‡ê²Œ ì‹¤ë ¥ ì—†ëŠ” ì‚¬ëŒì´ ìˆì—ˆë‹¤ë‹ˆ ğŸ™„\",\n",
    "    \"ì™œ ë°°í¬ ì´í›„ ì¥ì• ê°€ ìƒê²¼ëŠ”ì§€ ì•„ì§ë„ ëª¨ë¥¸ë‹¤ê³ ? ğŸ˜¡\",\n",
    "    \"ì™œ ì½”ë“œê°€ ëŠë¦°ì§€ ë„¤ê°€ ì„¤ëª… ì¢€ í•´ ì¤˜ ë´ ğŸŒ\",\n",
    "    \"ê·¸ ì •ë„ ì‹¤ë ¥ìœ¼ë¡œ ê°œë°œìëŠ” ë„ˆë¬´ ê³¼ë¶„í•˜ì§€ ì•Šë‚˜? ğŸ¤”\",\n",
    "    \"ì´ê±´ ë„¤ ì±…ì„ì´ë‹ˆê¹Œ ëë‚  ë•Œê¹Œì§€ ì§‘ì— ê°€ì§€ ë§ˆ ğŸ¢\",\n",
    "    \"ì™œ ê·¸ë ‡ê²Œ ê¸°ë³¸ì ì¸ ê±¸ ëª°ë¼? ğŸ“˜\",\n",
    "    \"ì£¼ë§ì— íšŒì‚¬ ì„œë²„ í„°ì¡ŒëŠ”ë° ì™œ ì—°ë½ ì•ˆ ë°›ì•˜ì–´? ğŸ“\",\n",
    "    \"ë°°í¬ ì „ í…ŒìŠ¤íŠ¸ë¥¼ ë¹¼ë¨¹ì—ˆë‹¤ë‹ˆ, ë„¤ê°€ ì§  ì½”ë“œëŠ” ì‹ ë¢°í•  ìˆ˜ ì—†ì–´ ğŸš§\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ê°€ ëŠë¦° ê±´ ë„¤ê°€ ì§  ì½”ë“œ ë•Œë¬¸ì´ì•¼ ğŸ’¥\",\n",
    "    \"ì™œ ë‹¤ë¥¸ íŒ€ì›ì´ í•­ìƒ ë„¤ ì½”ë“œë¥¼ ê³ ì³ì•¼ í•˜ì§€? ğŸ¤·â€â™‚ï¸\",\n",
    "    \"ë„¤ê°€ ë§Œë“  í•¨ìˆ˜ëŠ” ì§„ì§œ ë”ì°í•´ ğŸ˜¨\",\n",
    "    \"í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë„¤ ì½”ë“œëŠ” ì–¸ì œë‚˜ ì¥ì• ë¥¼ ìœ ë°œí•´ ğŸ”¥\",\n",
    "    \"ì´ ì •ë„ ê²°ê³¼ë¬¼ì´ë©´ ë” ì´ìƒ í•  ë§ì´ ì—†ë‹¤ ğŸ˜\",\n",
    "    \"í”„ë¡œê·¸ë˜ë°ì„ ë‹¤ì‹œ ë°°ì›Œì•¼ í•  ê²ƒ ê°™ì•„ ğŸ§‘â€ğŸ“\",\n",
    "    \"ì´ëŸ° ì‹ì´ë©´ ë” ì¢‹ì€ ê°œë°œìë¥¼ ì°¾ëŠ” ê²Œ ë‚«ê² ì–´ ğŸ•µï¸â€â™‚ï¸\",\n",
    "    \"ë„¤ê°€ ì½”ë“œë¥¼ ì§¤ ë•Œë§ˆë‹¤ QAê°€ ëŠ˜ì–´ë‚˜ ğŸ˜µâ€ğŸ’«\",\n",
    "    \"ì™œ ì•„ì§ë„ íšŒì‚¬ ë„ë©”ì¸ì„ ì´í•´ ëª» í•´? ğŸŒ\",\n",
    "    \"íŒ€ì— í”¼í•´ë¥¼ ì£¼ëŠ” ì½”ë“œë§Œ ì§œì§€ ë§ì•„ ì¤˜ ğŸš¨\",\n",
    "    \"ì§€ê¸ˆ ë„¤ê°€ í•˜ëŠ” ì¼ì€ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ ì‹œê°„ì„ ëºëŠ” ê±°ì•¼ â³\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ëŠ” ë„¤ê°€ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì‚¬ëŒì—ê²Œ ë§¡ê¸°ëŠ” ê²Œ ë‚˜ì•„ ğŸ˜¤\",\n",
    "    \"ë„¤ê°€ ë§Œë“  ëª¨ë“ˆì€ ì§„ì§œ ë¶ˆì•ˆì •í•˜ë‹¤ ğŸ¤¯\",\n",
    "    \"ë””ìì¸ íŒ¨í„´ì€ ì»¤ë…• ê¸°ë³¸ì ì¸ ë¡œì§ë„ ëª» ì§œ? ğŸ§\",\n",
    "    \"ìš°ë¦¬ íšŒì‚¬ ê¸°ìˆ  ìŠ¤íƒì´ ë„ˆí•œí…Œ ë„ˆë¬´ ì–´ë ¤ìš´ ê±´ê°€? ğŸ¤”\",\n",
    "    \"ë„¤ê°€ ì‘ì„±í•œ ì½”ë“œ ìŠ¤íƒ€ì¼ì€ ì§„ì§œ ìµœì•…ì´ì•¼ ğŸ˜©\",\n",
    "    \"ì™œ ì´ ê°„ë‹¨í•œ ë¬¸ì œë¥¼ ì•„ì§ë„ í•´ê²° ëª» í–ˆì–´? ğŸ•°ï¸\",\n",
    "    \"ì™œ ë„¤ ì½”ë“œì—ì„œë§Œ í•­ìƒ ë²„ê·¸ê°€ ë°œìƒí•˜ì§€? ğŸ\",\n",
    "\n",
    "    # ê³„ì† ë‹¤ë‹Œë‹¤ ë°œì–¸ (50ê°œ)\n",
    "    \"ë„¤ ì½”ë“œ ë•ë¶„ì— ì¥ì•  ì—†ì´ ë°°í¬ëë‹¤, ì •ë§ ê³ ë§ˆì›Œ! ğŸŒŸ\",\n",
    "    \"ë„¤ê°€ ë§Œë“  ê¸°ëŠ¥ì€ ì‚¬ìš©ìê°€ ì •ë§ ì¢‹ì•„í•  ê±°ì•¼ ğŸ˜Š\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ëŠ” ë„¤ê°€ ì•„ë‹ˆë©´ ë¶ˆê°€ëŠ¥í–ˆì–´ ğŸ†\",\n",
    "    \"ë„ˆì˜ ë””ë²„ê¹… ëŠ¥ë ¥ì€ ì •ë§ ëŒ€ë‹¨í•´ ğŸ”\",\n",
    "    \"ë„¤ê°€ ì‘ì„±í•œ í…ŒìŠ¤íŠ¸ëŠ” ì§„ì§œ ì² ì €í•˜ë”ë¼ âœ…\",\n",
    "    \"ì´ëŸ° UIë¥¼ ì§  ë„¤ê°€ íŒ€ì˜ ìë‘ì´ì•¼ ğŸ¨\",\n",
    "    \"ë°°í¬ í›„ì—ë„ ë²„ê·¸ í•˜ë‚˜ ì—†ë˜ ê²Œ ë‹¤ ë„¤ ë•ë¶„ì´ì•¼ ğŸŒˆ\",\n",
    "    \"íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì€ ë„¤ê°€ ë§Œë“¤ì–´ ì¤€ ê±°ì•¼ ğŸ’¡\",\n",
    "    \"ë„ˆ ì—†ìœ¼ë©´ íŒ€ì´ ëŒì•„ê°€ì§€ ì•Šì•„ ğŸ”„\",\n",
    "    \"í…Œí¬ ë¦¬ë”ê°€ ë„¤ ì‹¤ë ¥ì„ ì¸ì •í•˜ê³  ìˆì–´ ğŸ‘\",\n",
    "    \"ë„ˆì˜ ì•„ì´ë””ì–´ ë•ë¶„ì— í”„ë¡œì íŠ¸ê°€ í•œì¸µ ë‚˜ì•„ì¡Œì–´ ğŸ’¡\",\n",
    "    \"ë„¤ ì½”ë“œ ë¦¬ë·°ëŠ” ëª¨ë‘ì—ê²Œ í° ë„ì›€ì´ ë¼ ğŸ‘\",\n",
    "    \"ë„ˆì˜ ì»¤ë°‹ ë©”ì‹œì§€ëŠ” ì •ë§ ê¹”ë”í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›Œ ğŸ–‹ï¸\",\n",
    "    \"ë„¤ê°€ ì‘ì„±í•œ APIëŠ” ì§„ì§œ ì˜ˆìˆ ì´ì•¼ ğŸŒ\",\n",
    "    \"ë„ˆì˜ ë¡œì§ì€ í•­ìƒ íš¨ìœ¨ì ì´ê³  ë¹ ë¥´ë”ë¼ ğŸš€\",\n",
    "    \"ìš°ë¦¬ íŒ€ì— ë„¤ê°€ ìˆë‹¤ëŠ” ê²Œ í° í–‰ìš´ì´ì•¼ ğŸ€\",\n",
    "    \"ì‚¬ìš©ìë“¤ì´ ë„¤ê°€ ë§Œë“  ê¸°ëŠ¥ì„ ì¹­ì°¬í•˜ê³  ìˆì–´ ğŸ¯\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì‹œí‚¨ ê±´ ì „ì ìœ¼ë¡œ ë„¤ ê³µì´ì•¼ ğŸ…\",\n",
    "    \"ë„¤ê°€ ì‘ì„±í•œ ë¬¸ì„œëŠ” ëª¨ë‘ì—ê²Œ ë„ì›€ì´ ë¼ ğŸ“š\",\n",
    "    \"íšŒì‚¬ì—ì„œ ë„ˆ ê°™ì€ ê°œë°œìëŠ” ì°¾ê¸° ì–´ë ¤ì›Œ ğŸ› ï¸\",\n",
    "]\n",
    "\n",
    "# ë ˆì´ë¸” ìˆ˜ì •\n",
    "labels = [0] * 50 + [1] * 50\n",
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# ë¹ˆ ì‹œí€€ìŠ¤ í•„í„°ë§\n",
    "valid_sequences = []\n",
    "valid_labels = []\n",
    "for seq, label in zip(sequences, labels):\n",
    "    if len(seq) > 0:  # ë¹ˆ ì‹œí€€ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€\n",
    "        valid_sequences.append(seq)\n",
    "        valid_labels.append(label)\n",
    "\n",
    "# íŒ¨ë”© ì²˜ë¦¬\n",
    "x_train = pad_sequences(valid_sequences, maxlen=10)\n",
    "y_train = np.array(valid_labels)\n",
    "\n",
    "# ë°ì´í„° í¬ê¸° í™•ì¸\n",
    "print(f\"x_train í¬ê¸°: {x_train.shape}, y_train í¬ê¸°: {y_train.shape}\")\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=2000, output_dim=32, input_length=10),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"ë¸”ë™ê¸°ì—… ë°œì–¸ì— ë”°ë¥¸ ë°˜ì‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘!\")\n",
    "model.fit(x_train, y_train, epochs=30, verbose=1)\n",
    "\n",
    "# ë°˜ì‘ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_reaction(statement):\n",
    "    test_seq = tokenizer.texts_to_sequences([statement])\n",
    "    test_pad = pad_sequences(test_seq, maxlen=10)\n",
    "    prediction = model.predict(test_pad)[0][0]\n",
    "    if prediction > 0.5:\n",
    "        reaction = \"ê³„ì† ë‹¤ë‹Œë‹¤\"\n",
    "        humor = \"ì‚¬ì¥ë‹˜, ì¶©ì„±ì„ ë‹¤í•˜ê² ìŠµë‹ˆë‹¤! ğŸ™‡â€â™‚ï¸\"\n",
    "    else:\n",
    "        reaction = \"í‡´ì‚¬\"\n",
    "        humor = \"ì´ íšŒì‚¬, ë”ëŠ” ëª» ì°¸ê² ë‹¤! ë‹¹ì¥ í‡´ì‚¬í•©ë‹ˆë‹¤! ğŸƒâ€â™€ï¸\"\n",
    "    print(f\"'{statement}' => ì§ì› ë°˜ì‘: {reaction} (ì ìˆ˜: {prediction:.2f})\\n{humor}\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "test_statements = [\n",
    "    \"ë„¤ê°€ ë§Œë“  UIëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ë§ì¹˜ê³  ìˆì–´ ğŸ“‰\",\n",
    "    \"ì´ í”„ë¡œì íŠ¸ëŠ” ë„¤ê°€ ì•„ë‹ˆë©´ ë¶ˆê°€ëŠ¥í–ˆì–´ ğŸ†\",\n",
    "    \"ì ì€ ì£½ì–´ì„œ ìë„ ì¶©ë¶„í•´\",\n",
    "    \"íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì€ ë„¤ê°€ ë§Œë“¤ì–´ ì¤€ ê±°ì•¼ ğŸ’¡\",\n",
    "    \"ê²°ê³¼ë¬¼ì´ ì´ ì •ë„ë¼ë‹ˆ, ì‹¤ë§ìŠ¤ëŸ¬ì›Œ ğŸ˜”\",\n",
    "    \"ë„¤ ì½”ë“œ ë•ë¶„ì— ì¥ì•  ì—†ì´ ë°°í¬ëë‹¤, ì •ë§ ê³ ë§ˆì›Œ! ğŸŒŸ\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== ë¸”ë™ê¸°ì—… ì‚¬ì¥ì˜ ë°œì–¸ ì˜ˆì¸¡ ===\")\n",
    "for statement in test_statements:\n",
    "    predict_reaction(statement)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
